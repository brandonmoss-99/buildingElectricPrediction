{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "buildingEnergyPrediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzjLO-WyPT5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import Google Drive functionality & mount Google Drive to access datasets\n",
        "from google.colab import drive\n",
        "#import\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.impute import KNNImputer\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import time\n",
        "import random\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.python.keras.layers import Dense, Masking, LSTM, CuDNNLSTM, Dropout\n",
        "from tensorflow.python.keras import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import optimizers\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yDkcdiWf512",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read in data\n",
        "\n",
        "#### Data available at https://www.kaggle.com/c/ashrae-energy-prediction/data ####\n",
        "meterReadings = pd.read_csv('/content/drive/My Drive/colab/data/ashrae-energy-prediction/train.csv')\n",
        "buildingInfo = pd.read_csv('/content/drive/My Drive/colab/data/ashrae-energy-prediction/building_metadata.csv')\n",
        "weatherReadings = pd.read_csv('/content/drive/My Drive/colab/data/ashrae-energy-prediction/weather_train.csv')\n",
        "#### Data available at https://www.kaggle.com/c/ashrae-energy-prediction/data ####\n",
        "\n",
        "#convert meter reading zero values to NaN\n",
        "meterReadings.meter_reading.replace(0, np.nan, inplace=True)\n",
        "\n",
        "#sanity check. Are any buildings built during/after the data collection timeframe?\n",
        "newBuildings = buildingInfo.building_id.loc[buildingInfo.year_built >= 2016]\n",
        "#remove those buildings from data. Can't provide accurate prediction if they weren't fully built\n",
        "#or some data is from during construction, some after it was built. No data on day/month built to filter\n",
        "meterReadings = meterReadings.loc[~meterReadings.building_id.isin(newBuildings)]\n",
        "\n",
        "#convert timestamp values into datetime format for easier processing\n",
        "meterReadings['timestamp'] = pd.to_datetime(meterReadings['timestamp'], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "weatherReadings['timestamp'] = pd.to_datetime(weatherReadings['timestamp'], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "meterReadings['timestamp'] = meterReadings['timestamp'].dt.to_pydatetime()\n",
        "weatherReadings['timestamp'] = weatherReadings['timestamp'].dt.to_pydatetime()\n",
        "\n",
        "#convert building at site 0 from kBtu to kWh\n",
        "siteZero = buildingInfo['building_id'].loc[buildingInfo['site_id'] == 0].max()\n",
        "meterReadings.loc[meterReadings['building_id'] <= siteZero, ['meter_reading']] *= 0.293071\n",
        "\n",
        "#append the site_id of the building to each meter reading\n",
        "meterReadings.insert(1, 'site_id', \"\")\n",
        "for i in range (0,16):\n",
        "    maxBuilding = buildingInfo.loc[buildingInfo.site_id == i].building_id.max()\n",
        "    minBuilding = buildingInfo.loc[buildingInfo.site_id == i].building_id.min()\n",
        "    meterReadings.site_id[meterReadings.loc[(meterReadings.building_id >= minBuilding) & (meterReadings.building_id <= maxBuilding)].index] = i\n",
        "\n",
        "#convert 'primary_use' in buildingInfo to one hot encoding, for easier handling\n",
        "primaryUseOneHot = pd.get_dummies(buildingInfo['primary_use'], prefix='use', dummy_na=True)\n",
        "buildingInfo = pd.concat([buildingInfo, primaryUseOneHot], axis=1)\n",
        "\n",
        "#reduce the memory usage for each dataset\n",
        "def reduceDataMemUsage(data):\n",
        "    #datatypes used by numpy. No int8 as it's the smallest data type, no need to check if data fits in smaller type\n",
        "    dataTypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    for col in data.columns:\n",
        "        #get each column data type, check if in the list of possible types\n",
        "        colDataType = data[col].dtypes\n",
        "        if colDataType in dataTypes:\n",
        "            colMinValue = data[col].min()\n",
        "            colMaxValue = data[col].max()\n",
        "            #check cols default datatype, see if the min/max values allows all data to fit in a smaller data type\n",
        "            if str(colDataType)[:3] == 'int':\n",
        "                #if min/max of col fits in range of smaller data type, convert whole col to that data type\n",
        "                if colMinValue > np.iinfo(np.int8).min and colMaxValue < np.iinfo(np.int8).max:\n",
        "                    data[col] = data[col].astype(np.int8)\n",
        "                elif colMinValue > np.iinfo(np.int16).min and colMaxValue < np.iinfo(np.int16).max:\n",
        "                    data[col] = data[col].astype(np.int16)\n",
        "                elif colMinValue > np.iinfo(np.int32).min and colMaxValue < np.iinfo(np.int32).max:\n",
        "                    data[col] = data[col].astype(np.int32)\n",
        "                elif colMinValue > np.iinfo(np.int64).min and colMaxValue < np.iinfo(np.int64).max:\n",
        "                    data[col] = data[col].astype(np.int64)\n",
        "            else:\n",
        "                if colMinValue > np.finfo(np.float16).min and colMaxValue < np.finfo(np.float16).max:\n",
        "                    data[col] = data[col].astype(np.float16)\n",
        "                elif colMinValue > np.finfo(np.float32).min and colMaxValue < np.finfo(np.float32).max:\n",
        "                    data[col] = data[col].astype(np.float32)\n",
        "                else:\n",
        "                    data[col] = data[col].astype(np.float64)\n",
        "    return data\n",
        "\n",
        "#perform memory reduction function on each dataset\n",
        "for x in [meterReadings, weatherReadings, buildingInfo]:\n",
        "    x = reduceDataMemUsage(x)\n",
        "\n",
        "#create electricReadings of just electricity readings\n",
        "electricReadings = meterReadings[meterReadings['meter'] == 0]\n",
        "\n",
        "#get all of the unique timestamps in the dataset\n",
        "dates = electricReadings['timestamp'].unique()\n",
        "electricTrain = pd.DataFrame()\n",
        "electricTest = pd.DataFrame()\n",
        "\n",
        "# FOR 'SOFT RESET', the initial data doesn't have to be read in again for train/test operations\n",
        "electricTrain = pd.DataFrame()\n",
        "electricTest = pd.DataFrame()\n",
        "\n",
        "#split each 3 days of data into 2 days of training, 1 day of testing\n",
        "for i in range(0,8784,72):\n",
        "    electricTrain = electricTrain.append(electricReadings.loc[(electricReadings.timestamp >= dates[i]) & (electricReadings.timestamp <= dates[i+47])])\n",
        "    electricTest = electricTest.append(electricReadings.loc[(electricReadings.timestamp >= dates[i+48]) & (electricReadings.timestamp <= dates[i+71])])\n",
        "\n",
        "#drop all site 0 data from before 2016-05-21 00:00:00 - too complex to easily correct for missing data\n",
        "electricTrain.drop(electricTrain.loc[(electricTrain.site_id == 0) & (electricTrain.timestamp <= '2016-05-21 00:00:00')].index, inplace=True)\n",
        "electricTest.drop(electricTest.loc[(electricTest.site_id == 0) & (electricTest.timestamp <= '2016-05-21 00:00:00')].index, inplace=True)\n",
        "\n",
        "def outlierRemoval(df,x,focus,start,end):\n",
        "\t'''Perform Outlier Removal using a moving median\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tdf : DataFrame \n",
        "\t\tDataframe to use\n",
        "\tx : String \n",
        "\t\tDataframe column to remove outliers from\n",
        "\tfocus : String\n",
        "\t\tDataframe column to use for grouping data (by building, site etc)\n",
        "\tstart : int \n",
        "\t\tStarting value for group (building_id of 0)\n",
        "\tend : int\n",
        "\t\tEnding value for group (building_id of 1449)\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tFiltered\n",
        "\t\tA DataFrame with outliers removed\n",
        "\n",
        "\t'''\n",
        "\tFiltered = pd.DataFrame()\n",
        "\tfor i in range (start,end):\n",
        "\t    #get deep copy of data for each group\n",
        "\t    newList = df.loc[df[focus] == i].copy(deep=True)\n",
        "\t    #calculate median and standard deviation for each data window\n",
        "\t    newList['Rollingmedian_%s' %x] = newList[x].rolling(5).median()\n",
        "\t    newList['stdDev'] = newList[x].rolling(5).std()\n",
        "\t    #append filtered data to new dataframe (faster than modifying original dataframe)\n",
        "\t    Filtered = Filtered.append(newList)\n",
        "\treturn(Filtered)\n",
        " \n",
        "def fillMissingVals(df,x,focus,start,end):\n",
        "\t'''Perform imputation using Pandas' time interpolation\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tdf : DataFrame \n",
        "\t\tDataframe to use\n",
        "\tx : String \n",
        "\t\tDataframe column to impute\n",
        "\tfocus : String\n",
        "\t\tDataframe column to use for grouping data (by building, site etc)\n",
        "\tstart : int \n",
        "\t\tStarting value for group (building_id of 0)\n",
        "\tend : int\n",
        "\t\tEnding value for group (building_id of 1449)\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\timputed\n",
        "\t\tAn imputed DataFrame\n",
        "\n",
        "\t'''\n",
        "\timputed = pd.DataFrame()\n",
        "\tfor i in range(start,end):\n",
        "\t    toInterpolate = df.loc[df[focus] == i].copy(deep=True)\n",
        "\t    toInterpolate = toInterpolate.set_index('timestamp')\n",
        "\t    toInterpolate[x] = toInterpolate[x].interpolate(method='time')\n",
        "\t    imputed = imputed.append(toInterpolate)\n",
        "\timputed = imputed.reset_index()\n",
        "\treturn(imputed)\n",
        " \n",
        "def smoothVals(df,x,focus,start,end):\n",
        "\t'''Perform data smoothing using rolling mean\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tdf : DataFrame \n",
        "\t\tDataframe to use\n",
        "\tx : String \n",
        "\t\tDataframe column to smooth\n",
        "\tfocus : String\n",
        "\t\tDataframe column to use for grouping data (by building, site etc)\n",
        "\tstart : int \n",
        "\t\tStarting value for group (building_id of 0)\n",
        "\tend : int\n",
        "\t\tEnding value for group (building_id of 1449)\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tFiltered\n",
        "\t\tA DataFrame with additional row of smoothed data\n",
        "\n",
        "\t'''\n",
        "\trolling = pd.DataFrame()\n",
        "\tfor i in range(start,end):\n",
        "\t    #get deep copy of data for each group\n",
        "\t    vals = df.loc[df[focus] == i].copy(deep=True)\n",
        "\t    vals = vals.set_index('timestamp')\n",
        "\t    #calculate mean for each data window\n",
        "\t    vals['%sRollingAvg' %x] = vals[x].rolling(5, center=True, min_periods=1).mean()\n",
        "\t    rolling = rolling.append(vals)\n",
        "\trolling = rolling.reset_index()\n",
        "\treturn(rolling)\n",
        " \n",
        "\n",
        "## Code adapted from \n",
        "## Brownlee, J. 2019. How to Convert a Time Series to a Supervised Learning Problem in Python. [Source code]\n",
        "## Available at: https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
        "## [Accessed: 07 April 2020]\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\t\"\"\"\n",
        "\tFrame a time series as a supervised learning dataset.\n",
        "\tArguments:\n",
        "\t\tdata: Sequence of observations as a list or NumPy array.\n",
        "\t\tn_in: Number of lag observations as input (X).\n",
        "\t\tn_out: Number of observations as output (y).\n",
        "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
        "\tReturns:\n",
        "\t\tPandas DataFrame of series framed for supervised learning.\n",
        "\t\"\"\"\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = pd.DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = pd.concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg\n",
        "\n",
        "\n",
        "def evaluateModel():\n",
        "    #predict values with test data\n",
        "    predictedValues = model.predict(test_X)\n",
        "    #turn 3D results array back into 2D table form\n",
        "    test_X2 = test_X.reshape((test_X.shape[0], test_X.shape[1]*test_X.shape[2]))\n",
        "    #invert scaling for forecasted labels\n",
        "    invertedPredictedValues = np.concatenate((test_X2[:, -num_features:],predictedValues), axis=1)\n",
        "    #replace masked values with NaN before inverting\n",
        "    invertedPredictedValues[invertedPredictedValues == maskValue] = np.nan\n",
        "    invertedPredictedValues = scaler.inverse_transform(invertedPredictedValues)\n",
        "    invertedPredictedValues = invertedPredictedValues[:,-1]\n",
        "    #reshape & invert scaling for the actual test labels\n",
        "    test_y2 = test_y.reshape((len(test_y), 1))\n",
        "    invertedActualValues = np.concatenate((test_X2[:, -num_features:], test_y2), axis=1)\n",
        "    #replace masked values with NaN\n",
        "    invertedPredictedValues[invertedActualValues[:,-1] == maskValue] = np.nan\n",
        "    invertedActualValues[invertedActualValues == maskValue] = np.nan\n",
        "    invertedActualValues = scaler.inverse_transform(invertedActualValues)\n",
        "    invertedActualValues = invertedActualValues[:,-1]\n",
        "    #remove NaN values so RMSE can be calculated. Should remove the same values for prediction and actual\n",
        "    invertedPredictedValues = invertedPredictedValues[~np.isnan(invertedPredictedValues)]\n",
        "    invertedActualValues = invertedActualValues[~np.isnan(invertedActualValues)]\n",
        "    #calculate & return the root mean squared error between the prediction and actual values\n",
        "    return np.sqrt(mean_squared_error(invertedActualValues, invertedPredictedValues))\n",
        "\n",
        "#prepare site 2 data for machine learning\n",
        "electricTrain = outlierRemoval(electricTrain,'meter_reading','building_id',156,290)\n",
        "electricTest = outlierRemoval(electricTest,'meter_reading','building_id',156,290)\n",
        "\n",
        "#interpolate missing weather values\n",
        "weatherReadings = fillMissingVals(weatherReadings,'air_temperature','site_id',0,16)\n",
        "weatherReadings = fillMissingVals(weatherReadings,'sea_level_pressure','site_id',0,16)\n",
        "weatherReadings = fillMissingVals(weatherReadings,'wind_speed','site_id',0,16)\n",
        "\n",
        "#remove outlier weather values\n",
        "weatherReadings = outlierRemoval(weatherReadings,'air_temperature','site_id',0,16)\n",
        "weatherReadings = outlierRemoval(weatherReadings,'sea_level_pressure','site_id',0,16)\n",
        "weatherReadings = outlierRemoval(weatherReadings,'wind_speed','site_id',0,16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cULht8JEVji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "buildingStart = 156\n",
        "buildingEnd = 290\n",
        "past_hours = 24\n",
        "future_hours = 1\n",
        "num_features = 4\n",
        "num_observations = past_hours * num_features\n",
        "maskValue = -1\n",
        "\n",
        "electricTrainUseful = electricTrain[['building_id','site_id','Rollingmedian_meter_reading','timestamp']]\n",
        "electricTestUseful = electricTest[['building_id','site_id','Rollingmedian_meter_reading','timestamp']]\n",
        "#range of dates to reindex\n",
        "idx = pd.date_range('2016-01-01 00:00:00', '2016-12-31 23:00:00', freq='H')\n",
        "electricTrainReindexed = pd.DataFrame()\n",
        "electricTestReindexed = pd.DataFrame()\n",
        "#reindex every building's data to contain every date in the specified date range\n",
        "for i in range(buildingStart, buildingEnd):\n",
        "    #if training building has data\n",
        "    if len(electricTrainUseful.loc[electricTrainUseful['building_id'] == i].values) !=0:\n",
        "        electricTraining = electricTrainUseful.loc[electricTrainUseful.building_id == i].copy(deep=True)\n",
        "        bid = electricTraining[:1].values[0,0]\n",
        "        sid = electricTraining[:1].values[0,1]\n",
        "        #reindex\n",
        "        electricTraining = electricTraining.set_index('timestamp').reindex(idx).reset_index()\n",
        "        electricTraining['site_id'] = sid\n",
        "        electricTraining['building_id'] = bid\n",
        "        electricTrainReindexed = electricTrainReindexed.append(electricTraining)\n",
        "    #if testing building has data\n",
        "    if len(electricTestUseful.loc[electricTestUseful['building_id'] == i].values) !=0:\n",
        "        electricTesting2 = electricTestUseful.loc[electricTestUseful.building_id == i].copy(deep=True)\n",
        "        bid = electricTesting2[:1].values[0,0]\n",
        "        sid = electricTesting2[:1].values[0,1]\n",
        "        #reindex\n",
        "        electricTesting2 = electricTesting2.set_index('timestamp').reindex(idx).reset_index()\n",
        "        electricTesting2['site_id'] = sid\n",
        "        electricTesting2['building_id'] = bid\n",
        "        electricTestReindexed = electricTestReindexed.append(electricTesting2)\n",
        "\n",
        "#convert timedate64[ns] into hours elapsed since 2016-01-01T00:00:00, so it can go into the model as a float\n",
        "electricTrainReindexed['hoursElapsed'] = (electricTrainReindexed['index'] - np.datetime64('2016-01-01T00:00:00'))/np.timedelta64(1, 'h')\n",
        "electricTestReindexed['hoursElapsed'] = (electricTestReindexed['index'] - np.datetime64('2016-01-01T00:00:00'))/np.timedelta64(1, 'h')\n",
        "#merge data from other datasets\n",
        "electricTrainReindexed = electricTrainReindexed.merge(weatherReadings[['site_id', 'timestamp', 'Rollingmedian_air_temperature', 'Rollingmedian_sea_level_pressure']], how='left', left_on=['site_id', 'index'], right_on=['site_id', 'timestamp'])\n",
        "electricTrainReindexed = electricTrainReindexed.merge(buildingInfo[['building_id', 'site_id', 'square_feet']], how='left', on=['building_id', 'site_id'])\n",
        "electricTestReindexed = electricTestReindexed.merge(weatherReadings[['site_id', 'timestamp', 'Rollingmedian_air_temperature', 'Rollingmedian_sea_level_pressure']], how='left', left_on=['site_id', 'index'], right_on=['site_id', 'timestamp'])\n",
        "electricTestReindexed = electricTestReindexed.merge(buildingInfo[['building_id', 'site_id', 'square_feet']], how='left', on=['building_id', 'site_id'])\n",
        "#sort so all building data at given time are together\n",
        "electricTrainReindexed = electricTrainReindexed.sort_values(by=['index', 'building_id'])\n",
        "electricTestReindexed = electricTestReindexed.sort_values(by=['index', 'building_id'])\n",
        "#drop values that aren't floats, such as the timestamps which have been converted to hoursElapsed\n",
        "electricTrainReindexed = electricTrainReindexed[['hoursElapsed', 'square_feet', 'Rollingmedian_air_temperature', 'Rollingmedian_sea_level_pressure', 'Rollingmedian_meter_reading']]\n",
        "electricTestReindexed = electricTestReindexed[['hoursElapsed', 'square_feet', 'Rollingmedian_air_temperature', 'Rollingmedian_sea_level_pressure', 'Rollingmedian_meter_reading']]\n",
        "\n",
        "#set a whole row's data to NaN where its meter reading is NaN. \n",
        "#Will be replaced and ignored in the model\n",
        "electricTrainReindexed.loc[electricTrainReindexed.Rollingmedian_meter_reading.isnull()] = np.nan\n",
        "electricTestReindexed.loc[electricTestReindexed.Rollingmedian_meter_reading.isnull()] = np.nan\n",
        "\n",
        "#scale the values. Removes all column/dataframe info, only arrays from now on\n",
        "#fit scaler on the training data only, transform both with the training scaler parameters\n",
        "scaler = MinMaxScaler()\n",
        "electricTrainReindexedScaled = scaler.fit_transform(electricTrainReindexed)\n",
        "electricTestReindexedScaled = scaler.transform(electricTestReindexed)\n",
        "\n",
        "#replace all NaN values with mask value to be dropped in model\n",
        "#must happen after scaling otherwise mask value will be scaled with everything else\n",
        "electricTrainReindexedScaled = np.nan_to_num(electricTrainReindexedScaled, nan=maskValue)\n",
        "electricTestReindexedScaled = np.nan_to_num(electricTestReindexedScaled, nan=maskValue)\n",
        "\n",
        "#shift values to create x hours worth of data\n",
        "trainData = series_to_supervised(electricTrainReindexedScaled, past_hours, future_hours).values\n",
        "testData = series_to_supervised(electricTestReindexedScaled, past_hours, future_hours).values\n",
        "\n",
        "#split data into features & label (meter reading)\n",
        "colsToDelete = []\n",
        "futureColsToDelete = []\n",
        "#generate list of columns which contain each label for previous timesteps we don't want\n",
        "#want to delete the label for the first column (0 indexed so num_features addresses the label after the last feature)\n",
        "#and then up to the last past hour label\n",
        "for i in range(num_features,past_hours*(num_features+1),num_features+1):\n",
        "    colsToDelete.append(i)\n",
        "#list of columns for the future timesteps we don't want (all of the future's variables. Want only the labels for every future hour)\n",
        "for i in range(past_hours*(num_features+1), trainData.shape[1],num_features+1):\n",
        "    futureColsToDelete.extend(range(i,i+num_features))\n",
        "\n",
        "#delete all labels for past data & delete all observations for future data, split into _X and _y\n",
        "trainData = np.delete(trainData,futureColsToDelete, axis=1)\n",
        "trainData = np.delete(trainData,colsToDelete, axis=1)\n",
        "testData = np.delete(testData,futureColsToDelete, axis=1)\n",
        "testData = np.delete(testData,colsToDelete, axis=1)\n",
        "train_X, train_y = trainData[:,:-future_hours], trainData[:,-1]\n",
        "test_X, test_y = testData[:,:-future_hours], testData[:,-1]\n",
        "#FUTURE WORK. Set train_y/test_y to a series of labels for multiple future timesteps. Potentially sum together future labels into one\n",
        "#train_X, train_y = trainData[:,:-future_hours], trainData[:,num_observations:]\n",
        "#test_X, test_y = testData[:,:-future_hours], testData[:,num_observations:]\n",
        "\n",
        "#reshape features into a 3D array\n",
        "train_X = train_X.reshape(train_X.shape[0], 1, num_features*past_hours)\n",
        "test_X = test_X.reshape(test_X.shape[0], 1, num_features*past_hours)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KqScec_xUhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainModels(numNodes, numEpochs, batchSize, optimiserToUse, activationChosen, maskingEnabled, dropoutEnabled, dropoutValue, useGPU, model):\n",
        "    #design network\n",
        "    if(maskingEnabled):\n",
        "        model.add(Masking(mask_value=maskValue ,input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "        model.add(LSTM(numNodes))\n",
        "    else:\n",
        "        if(useGPU):\n",
        "            model.add(CuDNNLSTM(numNodes, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "        else:\n",
        "            model.add(LSTM(numNodes, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "    if(dropoutEnabled):\n",
        "        model.add(Dropout(dropoutValue))\n",
        "    model.add(Dense(1, activation=activationChosen))\n",
        "    model.compile(loss='mse', optimizer=optimiserToUse)\n",
        "    #fit network\n",
        "    history = model.fit(train_X, train_y, epochs=numEpochs, batch_size=batchSize, validation_split=0.33, verbose=0, shuffle=False)\n",
        "    return history\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "#used to conveniently replace words in plot labels & result outputs\n",
        "toModify = 'epochs'\n",
        "\n",
        "for n in [16]:\n",
        "    print('Running node_size %s'%n)\n",
        "    for i in [10,20,30,40,50,60,70,80,90,100]:\n",
        "        runValues = []\n",
        "        for j in range(1,4):\n",
        "            #run each experiment 3 times for an avg\n",
        "            model = Sequential()\n",
        "            optimiserToUse = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "            history = trainModels(n, i, past_hours*(buildingEnd-buildingStart), optimiserToUse, 'relu', False, True, 0.2, False, model)\n",
        "            result = evaluateModel()\n",
        "            runValues.append(result)\n",
        "            #print RMSE result in LaTeX Tabular friendly way\n",
        "            print('%.4f'%(result),sep='', end='&')\n",
        "            if(j==2):\n",
        "                #plot the last run only\n",
        "                plt.plot(history.history['loss'], label='Train %d %s'%(i, toModify), linestyle='-')\n",
        "                plt.plot(history.history['val_loss'], label='Val %d %s'%(i, toModify), linestyle='--')\n",
        "            tf.keras.backend.clear_session()\n",
        "        print('Avg for %s:%d RMSE:%.4f'%(toModify, i, np.average(runValues)))\n",
        "\n",
        "    plt.title('How the varying of %s affects training & validation loss'%toModify)\n",
        "    plt.xlabel('Number of %s'%toModify)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.09),\n",
        "            fancybox=True, shadow=False, ncol=5)\n",
        "    #plt.savefig('/content/drive/My Drive/colab/varyingEpochsCPU_%sNodesReluDropout0_2v2.png'%n, bbox_inches='tight')\n",
        "    #plt.clf()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjbUnXHeT14c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict test set on the model, invert results\n",
        "\n",
        "#predict values with test data\n",
        "predictedValues = model.predict(test_X)\n",
        "\n",
        "testDataReindexed = np.nan_to_num(electricTestReindexed, nan=maskValue)\n",
        "testDataShifted = series_to_supervised(testDataReindexed, past_hours, future_hours).values\n",
        "#filter to only the values for each timestep\n",
        "testDataValueSequence = testDataShifted[:,colsToDelete]\n",
        "#turn 3D results array back into 2D table form\n",
        "test_X2 = test_X.reshape((test_X.shape[0], test_X.shape[1]*test_X.shape[2]))\n",
        "#invert scaling for forecasted labels\n",
        "invertedPredictedValues = np.concatenate((test_X2[:, -num_features:],predictedValues), axis=1)\n",
        "#replace masked values with NaN before inverting\n",
        "invertedPredictedValues[invertedPredictedValues == maskValue] = np.nan\n",
        "invertedPredictedValues = scaler.inverse_transform(invertedPredictedValues)\n",
        "invertedPredictedValues = invertedPredictedValues[:,-1]\n",
        "#reshape & invert scaling for the actual test labels\n",
        "test_y2 = test_y.reshape((len(test_y), 1))\n",
        "invertedActualValues = np.concatenate((test_X2[:, -num_features:], test_y2), axis=1)\n",
        "#replace masked values with NaN\n",
        "invertedPredictedValues[invertedActualValues[:,-1] == maskValue] = np.nan\n",
        "invertedActualValues[invertedActualValues == maskValue] = np.nan\n",
        "invertedActualValues = scaler.inverse_transform(invertedActualValues)\n",
        "invertedActualValues = invertedActualValues[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnWy7UqfuV65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot random sequence of readings. May need to run several times to get \n",
        "# a sequence plot that isn't just the masking value\n",
        "randomValue = random.randint(0, invertedActualValues.shape[0]-1)\n",
        "sequenceToPlot = testDataValueSequence[randomValue,:]\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(sequenceToPlot, label='Prior sequence meter readings')\n",
        "plt.plot([24],invertedPredictedValues[randomValue], marker='x', label='Predicted meter reading')\n",
        "plt.plot([24],invertedActualValues[randomValue], marker='o', label='Actual meter reading')\n",
        "\n",
        "futurePredictions = []\n",
        "futureActuals = []\n",
        "for i in range(1,24):\n",
        "    plt.plot([24+i],invertedPredictedValues[randomValue+i], marker='x', label='Predicted+%s meter reading'%i)\n",
        "    plt.plot([24+i],invertedActualValues[randomValue+i], marker='o', label='Actual+%s meter reading'%i)\n",
        "\n",
        "plt.title('Predicting the next 24 meter readings of test data sequence %d' %randomValue)\n",
        "plt.xlabel('Point in sequence')\n",
        "plt.ylabel('Meter Reading (kWh)')\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.09),\n",
        "        fancybox=True, shadow=False, ncol=5)\n",
        "\n",
        "#plt.savefig('/content/drive/My Drive/colab/sequenceFuture24PredictionGraph_%s.png'%randomValue, bbox_inches='tight')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqMTprHcb-vZ",
        "colab_type": "text"
      },
      "source": [
        "Below are some snippets of code not needed to run the current solution, but were used at one point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfGrrSx3JCq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create heatmap of absolute Pearson correlation between all dataset columns\n",
        "merged = pd.merge(electricTrain, weatherReadings)\n",
        "merged = pd.merge(merged, buildingInfo)\n",
        "plt.figure(figsize=(10,8))\n",
        "testMatrix = merged.corr().abs()\n",
        "sns.heatmap(testMatrix)\n",
        "plt.title('Absolute Pearson correlation heatmap between the \\n electricTrain, weatherReadings and buildingInfo dataframes')\n",
        "#plt.savefig('/content/drive/My Drive/colab/corrHeatmap.png', bbox_inches='tight')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u80BbRj9bkt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict test set on the model, print RMSE & random sample of predictions vs actual values\n",
        "\n",
        "#predict values with test data\n",
        "predictedValues = model.predict(test_X)\n",
        "#turn 3D results array back into 2D table form\n",
        "test_X2 = test_X.reshape((test_X.shape[0], test_X.shape[1]*test_X.shape[2]))\n",
        "#invert scaling for forecasted labels\n",
        "invertedPredictedValues = np.concatenate((test_X2[:, -num_features:],predictedValues), axis=1)\n",
        "#replace masked values with NaN before inverting\n",
        "invertedPredictedValues[invertedPredictedValues == maskValue] = np.nan\n",
        "invertedPredictedValues = scaler.inverse_transform(invertedPredictedValues)\n",
        "invertedPredictedValues = invertedPredictedValues[:,-1]\n",
        "#reshape & invert scaling for the actual test labels\n",
        "test_y2 = test_y.reshape((len(test_y), 1))\n",
        "invertedActualValues = np.concatenate((test_X2[:, -num_features:], test_y2), axis=1)\n",
        "#replace masked values with NaN\n",
        "invertedPredictedValues[invertedActualValues[:,-1] == maskValue] = np.nan\n",
        "invertedActualValues[invertedActualValues == maskValue] = np.nan\n",
        "invertedActualValues = scaler.inverse_transform(invertedActualValues)\n",
        "invertedActualValues = invertedActualValues[:,-1]\n",
        "#remove NaN values so RMSE can be calculated. Should remove the same values for prediction and actual\n",
        "invertedPredictedValues = invertedPredictedValues[~np.isnan(invertedPredictedValues)]\n",
        "invertedActualValues = invertedActualValues[~np.isnan(invertedActualValues)]\n",
        "#calculate & return the root mean squared error between the prediction and actual values\n",
        "rmse = np.sqrt(mean_squared_error(invertedActualValues, invertedPredictedValues))\n",
        "print('Test RMSE: %.5f' % rmse)\n",
        "#print random sample of prediction and actual values\n",
        "print('\\nRandom sample of predictions vs actual values:')\n",
        "for i in range(0,10):\n",
        "    randomValue = random.randint(0, invertedActualValues.shape[0]-1)\n",
        "    print('value %7d, predicted: %.5f, actual: %.5f, difference: %.5f (%.2f%%)' % (randomValue, invertedPredictedValues[randomValue], \n",
        "                                                                          invertedActualValues[randomValue], \n",
        "                                                                          (invertedPredictedValues[randomValue] - invertedActualValues[randomValue]),\n",
        "                                                                          ((invertedPredictedValues[randomValue] - invertedActualValues[randomValue])/abs(invertedActualValues[randomValue])*100)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_h5xEQFWdSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualise rolling meter readings\n",
        "#electricTrain = electricTrain.reset_index()\n",
        "IdToProcess = 231\n",
        "data = electricTrainTest.loc[electricTrainTest.building_id == 231]\n",
        "print(data)\n",
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "ax.plot(data['index'], data.Rollingmedian)\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Electricity Usage (kWh)')\n",
        "ax.set_title('Building ID: %s \\nRolling median meter readings (window size 5)' %IdToProcess)\n",
        "#plt.savefig('/content/drive/My Drive/colab/graphics/trainBuilding' + str(IdToProcess) + '_rollingmedian_5.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5uRRDpbqmbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate how much data/percent of data is missing from each dataset, printed into LaTeX tables\n",
        "def calcMissing:\n",
        "    totalMissing = weatherReadings.isnull().sum().sort_values(ascending=False)\n",
        "    percentMissing = (weatherReadings.isnull().sum()/len(weatherReadings.index)*100).sort_values(ascending=False)\n",
        "    missingweatherReadingsData = pd.concat([totalMissing, percentMissing], keys=['Total Values Missing', 'Percent of Values Missing'], axis=1)\n",
        "    #print(missingweatherReadingsData.to_latex(index=True))\n",
        "\n",
        "    totalMissing = meterReadings.isnull().sum().sort_values(ascending=False)\n",
        "    percentMissing = (meterReadings.isnull().sum()/len(meterReadings.index)*100).sort_values(ascending=False)\n",
        "    missingmeterReadingsData = pd.concat([totalMissing, percentMissing], keys=['Total Values Missing', 'Percent of Values Missing'], axis=1)\n",
        "    #print(missingmeterReadingsData.to_latex(index=True))\n",
        "\n",
        "    totalMissing = buildingInfo.isnull().sum().sort_values(ascending=False)\n",
        "    percentMissing = (buildingInfo.isnull().sum()/len(buildingInfo.index)*100).sort_values(ascending=False)\n",
        "    missingBuildingInfoData = pd.concat([totalMissing, percentMissing], keys=['Total Values Missing', 'Percent of Values Missing'], axis=1)\n",
        "    #print(missingBuildingInfoData.to_latex(index=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pde0kimeUeZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualise how much data exists, and whether it is NaN, or zero\n",
        "meterReadings = meterReadings.set_index(['timestamp'])\n",
        "f, a=plt.subplots(1, 4, figsize=(23, 30))\n",
        "for meterType in np.arange(4): #for each type of meter\n",
        "    converted = meterReadings[meterReadings['meter'] == meterType].copy().reset_index() #copy only the same meter type values to df, and reset the index to default\n",
        "    converted['timestamp'] = (pd.to_timedelta(converted['timestamp']).dt.total_seconds() / 3600).astype(int) #convert the timestamp col values to hours since start of timestamp (1677-09-21)\n",
        "    converted['timestamp'] -= converted['timestamp'].min() #subtract the min time supported by timestamp (1677-09-21) to get the time since 1st value\n",
        "    missingValues = np.empty((1449, converted['timestamp'].max() + 1)) #create new empty dataframe of size 1449 x max number in timestamp (each building's entire readings per row)\n",
        "    missingValues.fill(np.nan) #fill dataframe with NaN values\n",
        "    for meterReading in converted.values:\n",
        "        missingValues[int(meterReading[1]), int(meterReading[0])] = 0 if meterReading[4] == 0 else 1 #set dataframe at row building No, column hour elapsed with 0 if value is 0, 1 if non-0 value exists\n",
        "    a[meterType].set_title(f'meter {meterType:d}')\n",
        "    sns.heatmap(missingValues, cmap='Paired', ax=a[meterType], cbar=False)\n",
        "meterReadings = meterReadings.reset_index()\n",
        "#plt.savefig('/content/drive/My Drive/colab/meterReadingsUnprocessed.png', bbox_inches='tight')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NKmHl_kzc6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualise how much electricity data exists, and whether it is NaN, or zero\n",
        "#electricReadings = electricReadings.set_index(['timestamp'])\n",
        "converted = electricTrain.copy() #copy only the same meter type values to df, and reset the index to default\n",
        "converted['timestamp'] = (pd.to_timedelta(converted['timestamp']).dt.total_seconds() / 3600).astype(int) #convert the timestamp col values to hours since start of timestamp (1677-09-21)\n",
        "converted['timestamp'] -= converted['timestamp'].min() #subtract the min time supported by timestamp (1677-09-21) to get the time since 1st value\n",
        "missingValues = np.empty((1449, converted['timestamp'].max() + 1)) #create new empty dataframe of size 1449 x max number in timestamp (each building's entire readings per row)\n",
        "missingValues.fill(np.nan) #fill dataframe with NaN values\n",
        "for electricReading in converted.values:\n",
        "    missingValues[int(electricReading[0]), int(electricReading[3])] = 0 if electricReading[4] == 0 else 1 #set dataframe at row building No, column hour elapsed with 0 if value is 0, 1 if non-0 value exists\n",
        "sns.set(rc={'figure.figsize':(20, 30)})\n",
        "plot = sns.heatmap(missingValues, cmap='Paired', cbar=False)\n",
        "plt.title('Electricity training data visualisation', loc='center')\n",
        "plt.xlabel('Hours from start of data')\n",
        "plt.ylabel('Building ID')\n",
        "#electricReadings = electricReadings.reset_index() #reset index after done, to allow other functions to work on the original dataset shape\n",
        "#plt.savefig('/content/drive/My Drive/colab/electricityTrainUnprocessed.png', bbox_inches='tight')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb40fscM3m2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Code adapted from\n",
        "## HernÃ¡ndez, J. 2019. ASHRAE - Outliers. [Source code]\n",
        "## Available at: https://www.kaggle.com/juanmah/ashrae-outliers/notebook\n",
        "## [Accessed: 24 February 2020]\n",
        "\n",
        "electricTrainDaily = electricTrain\n",
        "#group electric readings by date and then building ID. Sum all columns (though only interested in meter readings)\n",
        "electricTrainDaily = electricTrainDaily.groupby([electricTrain.timestamp.dt.date, 'building_id']).sum()\n",
        "#combine the hourly measurements within each date, providing the sum, mean, which index has max value & max value for each\n",
        "electricTrainDailyAgg = electricTrainDaily.groupby(['timestamp']).agg(['sum', 'mean', 'idxmax', 'max'])\n",
        "#reduce dimensions of dataframe, easier to access values\n",
        "level_0 = electricTrainDailyAgg.columns.droplevel(0)\n",
        "level_1 = electricTrainDailyAgg.columns.droplevel(1)\n",
        "#change namings of headings\n",
        "level_0 = ['' if x == '' else '-' + x for x in level_0]\n",
        "electricTrainDailyAgg.columns = level_1 + level_0\n",
        "electricTrainDailyAgg.rename_axis(None, axis=1)\n",
        "#create list of which building_id uses maximum electricity for each date\n",
        "dailyMaxElectricConsumers = [building[1] for building in electricTrainDailyAgg['meter_reading-idxmax']]\n",
        "#count no. times each building_id appears in list, which building has most days of maximum consumption\n",
        "from collections import Counter\n",
        "print('Building ID: No. times max consumer:',Counter(dailyMaxElectricConsumers))\n",
        "#print('Building ID: No. times max consumer:',Counter(dailyMaxElectricConsumers).to_latex(index=True))\n",
        "#find out what type of buildings these max consumers are\n",
        "print(buildingInfo.primary_use.loc[buildingInfo.building_id.isin(dailyMaxElectricConsumers)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0oXzVG3YrkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Robust Z Score method. Too slow in current implementation\n",
        "\n",
        "def getRobustZScore(x):\n",
        "    #get the median of the sorted values. Numpy automatically sorts values\n",
        "    windowMedian = np.median(x)\n",
        "    #calculate Median Absolute Deviation\n",
        "    medianAbsDev = np.median([np.abs(x - windowMedian) for windowSortedVal in x])\n",
        "    #calculate z score\n",
        "    robustZScore = 0.6745 * np.sum([x - windowMedian for windowSortedVal in x]) / medianAbsDev\n",
        "    return np.abs(robustZScore)\n",
        "\n",
        "for i in range(1,10):\n",
        "    buildingList = electricTrain.loc[electricTrain.building_id == i].copy()\n",
        "    buildingList['rZScore'] = buildingList.meter_reading.rolling(5).apply(getRobustZScore, raw=True)\n",
        "#electricTrain.drop(buildingList.loc[buildingList.rZScore >= 3.5].index, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMuagcc-R2eW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the missing values\n",
        "electricReadings.reset_index()\n",
        "totalMissing = electricReadings.isnull().sum().sort_values(ascending=False)\n",
        "percentMissing = (electricReadings.isnull().sum()/len(electricReadings.index)*100).sort_values(ascending=False)\n",
        "missingelectricReadingsData = pd.concat([totalMissing, percentMissing], keys=['Total Values Missing', 'Percent of Values Missing'], axis=1)\n",
        "print(missingelectricReadingsData.to_latex(index=True))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}